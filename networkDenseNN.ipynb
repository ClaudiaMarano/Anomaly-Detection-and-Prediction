{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOvx+9zDnuSZ+cJx+aJASK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudiaMarano/Anomaly-Detection-and-Prediction/blob/main/networkDenseNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo è il task di prediction del progetto di Anomaly-Detection-and-Prediction relativo alla parte di Network. Il metodo su cui si basa questo task è quello basato sull'utilizzo di Dense NN\n"
      ],
      "metadata": {
        "id": "8BzKxTmbJ3PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline per il preprocessing dei dati\n",
        "--------------------------------------\n",
        "IMPORTANTE\n",
        "--------------------------------------\n",
        "Ad ogni runtime aggiungere nuovamente i file csv in una cartella chiamata csv, non è possibile caricare tutti i csv poichè troppo pesanti"
      ],
      "metadata": {
        "id": "8LZDLmU4NiV_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyP-Px0kJwzu",
        "outputId": "39d0def1-b7df-490a-ef16-943b6e38a490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature comuni a tutti i file:\n",
            "{'sport', 'proto', 'modbus_response', 'label_n', 'Time', 'ip_d', 'dport', 'mac_d', 'flags', 'mac_s', 'n_pkt_dst', 'modbus_fn', 'ip_s', 'label', 'n_pkt_src', 'size'}\n",
            "Feature uniche per ciascun file:\n",
            "File: attack_4.csv\n",
            "Feature uniche: set()\n",
            "File: attack_2.csv\n",
            "Feature uniche: set()\n",
            "File: attack_1.csv\n",
            "Feature uniche: set()\n",
            "File: attack_3.csv\n",
            "Feature uniche: set()\n",
            "File: normal.csv\n",
            "Feature uniche: set()\n",
            "Unione completata. Numero totale di righe: 953689\n",
            "Analisi delle feature:\n",
            "Numero di valori unici per feature:\n",
            "Time               913601\n",
            "mac_s                   9\n",
            "mac_d                  11\n",
            "ip_s                    8\n",
            "ip_d                    8\n",
            "sport                2291\n",
            "dport                2291\n",
            "proto                   3\n",
            "flags                   7\n",
            "size                   30\n",
            "modbus_fn               5\n",
            "modbus_response      1566\n",
            "n_pkt_src              54\n",
            "n_pkt_dst              54\n",
            "label_n                 2\n",
            "label                   4\n",
            "dtype: int64\n",
            "Valori mancanti per feature:\n",
            "Time                    0\n",
            "mac_s                   0\n",
            "mac_d                   1\n",
            "ip_s                   68\n",
            "ip_d                   68\n",
            "sport                  68\n",
            "dport                  68\n",
            "proto                   3\n",
            "flags                  68\n",
            "size                    3\n",
            "modbus_fn           33460\n",
            "modbus_response    493549\n",
            "n_pkt_src              69\n",
            "n_pkt_dst              69\n",
            "label_n                 5\n",
            "label                   5\n",
            "dtype: int64\n",
            "Feature con valori costanti:\n",
            "[]\n",
            "Feature 'label' rimossa con successo.\n",
            "Feature 'modbus_response' rimossa con successo.\n",
            "Feature 'modbus_fn' rimossa con successo.\n",
            "Valori mancanti eliminati. Dimensione iniziale: (953689, 13), Dimensione dopo eliminazione: (953619, 13).\n",
            "Analisi delle feature:\n",
            "Numero di valori unici per feature:\n",
            "Time         913532\n",
            "mac_s             8\n",
            "mac_d             8\n",
            "ip_s              8\n",
            "ip_d              8\n",
            "sport          2291\n",
            "dport          2291\n",
            "proto             2\n",
            "flags             7\n",
            "size             30\n",
            "n_pkt_src        54\n",
            "n_pkt_dst        54\n",
            "label_n           2\n",
            "dtype: int64\n",
            "Valori mancanti per feature:\n",
            "Time         0\n",
            "mac_s        0\n",
            "mac_d        0\n",
            "ip_s         0\n",
            "ip_d         0\n",
            "sport        0\n",
            "dport        0\n",
            "proto        0\n",
            "flags        0\n",
            "size         0\n",
            "n_pkt_src    0\n",
            "n_pkt_dst    0\n",
            "label_n      0\n",
            "dtype: int64\n",
            "Feature con valori costanti:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Funzione per controllare differenze nelle feature tra i file CSV\n",
        "def check_feature_differences(file_paths):\n",
        "    feature_sets = []\n",
        "\n",
        "    for file in file_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(file, nrows=1)  # Legge solo la prima riga per ottenere le feature\n",
        "            df.columns = df.columns.str.strip()  # Rimuove spazi dai nomi delle colonne\n",
        "            feature_sets.append(set(df.columns))\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante la lettura del file {file}: {e}\")\n",
        "\n",
        "    # Confronta le feature tra i file\n",
        "    common_features = set.intersection(*feature_sets)  # Feature comuni a tutti i file\n",
        "    unique_features = [features - common_features for features in feature_sets]  # Feature uniche per file\n",
        "\n",
        "    # Output dei risultati\n",
        "    result = {\n",
        "        \"common_features\": common_features,\n",
        "        \"unique_features_per_file\": {\n",
        "            os.path.basename(file): unique for file, unique in zip(file_paths, unique_features)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Funzione per unire i file CSV\n",
        "def merge_csv_files(file_paths):\n",
        "    dataframes = []\n",
        "\n",
        "    for file in file_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df.columns = df.columns.str.strip()  # Rimuove spazi dai nomi delle colonne\n",
        "            dataframes.append(df)\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"File vuoto: {file}\")\n",
        "        except pd.errors.ParserError as e:\n",
        "            print(f\"Errore di parsing nel file {file}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore sconosciuto durante la lettura del file {file}: {e}\")\n",
        "\n",
        "    try:\n",
        "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "        print(f\"Unione completata. Numero totale di righe: {combined_df.shape[0]}\")\n",
        "        return combined_df\n",
        "    except ValueError as e:\n",
        "        print(f\"Errore durante l'unione dei file: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Funzione per analizzare le feature e identificare quelle inutili\n",
        "def analyze_features(df):\n",
        "    try:\n",
        "        # Calcolo del numero di valori unici per ogni colonna\n",
        "        unique_counts = df.nunique()\n",
        "\n",
        "        # Identificazione di feature con valori mancanti\n",
        "        missing_values = df.isna().sum()\n",
        "\n",
        "        # Identificazione di feature con valori costanti\n",
        "        constant_features = [col for col in df.columns if unique_counts[col] == 1]\n",
        "\n",
        "        # Report\n",
        "        print(\"Analisi delle feature:\")\n",
        "        print(\"Numero di valori unici per feature:\")\n",
        "        print(unique_counts)\n",
        "        print(\"Valori mancanti per feature:\")\n",
        "        print(missing_values)\n",
        "        print(\"Feature con valori costanti:\")\n",
        "        print(constant_features)\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'analisi delle feature: {e}\")\n",
        "\n",
        "# Funzione per rimuovere la feature 'label'\n",
        "def remove_label_feature(df):\n",
        "    try:\n",
        "        if 'label' in df.columns:\n",
        "            df = df.drop(columns=['label'])\n",
        "            print(\"Feature 'label' rimossa con successo.\")\n",
        "        else:\n",
        "            print(\"Feature 'label' non trovata nel dataset.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la rimozione della feature 'label': {e}\")\n",
        "        return df\n",
        "\n",
        "# Funzione per rimuovere la feature 'modbus_response'\n",
        "def remove_modbus_response_feature(df):\n",
        "    try:\n",
        "        if 'modbus_response' in df.columns:\n",
        "            df = df.drop(columns=['modbus_response'])\n",
        "            print(\"Feature 'modbus_response' rimossa con successo.\")\n",
        "        else:\n",
        "            print(\"Feature 'modbus_response' non trovata nel dataset.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la rimozione della feature 'modbus_response': {e}\")\n",
        "        return df\n",
        "\n",
        "\n",
        "# Funzione per rimuovere la feature 'modbus_fn'\n",
        "def remove_modbus_fn_feature(df):\n",
        "    try:\n",
        "        if 'modbus_fn' in df.columns:\n",
        "            df = df.drop(columns=['modbus_fn'])\n",
        "            print(\"Feature 'modbus_fn' rimossa con successo.\")\n",
        "        else:\n",
        "            print(\"Feature 'modbus_fn' non trovata nel dataset.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la rimozione della feature 'modbus_fn': {e}\")\n",
        "        return df\n",
        "\n",
        "\n",
        "# Funzione per eliminare i valori mancanti dal dataset\n",
        "def drop_missing_values(df):\n",
        "    try:\n",
        "        initial_shape = df.shape\n",
        "        df = df.dropna()\n",
        "        print(f\"Valori mancanti eliminati. Dimensione iniziale: {initial_shape}, Dimensione dopo eliminazione: {df.shape}.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'eliminazione dei valori mancanti: {e}\")\n",
        "        return df\n",
        "\n",
        "# Directory del dataset Network\n",
        "dataset_path = \"./csv\"\n",
        "network_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
        "\n",
        "# Controllo delle differenze nelle feature tra i file\n",
        "feature_differences = check_feature_differences(network_files)\n",
        "\n",
        "# Output dei risultati\n",
        "print(\"Feature comuni a tutti i file:\")\n",
        "print(feature_differences[\"common_features\"])\n",
        "\n",
        "print(\"Feature uniche per ciascun file:\")\n",
        "for file, unique_features in feature_differences[\"unique_features_per_file\"].items():\n",
        "    print(f\"File: {file}\")\n",
        "    print(f\"Feature uniche: {unique_features}\")\n",
        "\n",
        "# Unione dei file CSV\n",
        "combined_dataset = merge_csv_files(network_files)\n",
        "\n",
        "# Analisi delle feature\n",
        "analyze_features(combined_dataset)\n",
        "\n",
        "# Rimozione della feature 'label'\n",
        "combined_dataset = remove_label_feature(combined_dataset)\n",
        "\n",
        "# Rimozione della feature 'modbus_response'\n",
        "combined_dataset = remove_modbus_response_feature(combined_dataset)\n",
        "\n",
        "# Rimozione della feature 'modbus_fn'\n",
        "combined_dataset = remove_modbus_fn_feature(combined_dataset)\n",
        "\n",
        "# Eliminazione dei valori mancanti\n",
        "combined_dataset = drop_missing_values(combined_dataset)\n",
        "\n",
        "combined_dataset.head()\n",
        "\n",
        "# Analisi delle feature\n",
        "analyze_features(combined_dataset)"
      ]
    }
  ]
}