{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudiaMarano/Anomaly-Detection-and-Prediction/blob/main/network_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Physical Anomaly Detection"
      ],
      "metadata": {
        "id": "u9jQDQ_pNV2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Librerie"
      ],
      "metadata": {
        "id": "xs62gvzhNhkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n"
      ],
      "metadata": {
        "id": "mtAgZvi2dUxT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caricamento e Preprocessing\n",
        "Il dataset viene suddiviso in intervalli di un minuto in modo da addestrare la rete su intervalli di campioni senza anomalie, in modo da poter riconoscere in fase di test quando un intervallo contiene invece un'anomalia.\n"
      ],
      "metadata": {
        "id": "4SamnLoTgwII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caricamento e Divisione Dataset"
      ],
      "metadata": {
        "id": "HME4FvAZkv8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_data(path):\n",
        "    # Caricamento del dataset\n",
        "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
        "\n",
        "    # Assicuriamoci che 'Time' sia in formato datetime\n",
        "    df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
        "\n",
        "    # Rimuovi righe con valori di data non validi\n",
        "    df = df.dropna(subset=['Time'])\n",
        "\n",
        "    # Ordinare i dati per timestamp\n",
        "    df_normal = df.sort_values(by='Time')\n",
        "\n",
        "    # Divisione in intervalli di un minuto\n",
        "    segments = []\n",
        "    window_duration = pd.Timedelta(milliseconds=100)\n",
        "    start_time = df_normal['Time'].iloc[0]\n",
        "\n",
        "    print(\"Start Time: \", df_normal['Time'].iloc[0])\n",
        "    print(\"Finish Time: \", df_normal['Time'].iloc[-1])\n",
        "\n",
        "    while start_time < df_normal['Time'].iloc[-1]:\n",
        "        end_time = start_time + window_duration\n",
        "        segment = df_normal[(df_normal['Time'] >= start_time) & (df_normal['Time'] < end_time)]\n",
        "        if len(segment) > 0:\n",
        "            segments.append(segment.drop(columns=['Time', 'label', 'label_n', 'modbus_response']).reset_index(drop=True).values)\n",
        "        start_time = end_time\n",
        "\n",
        "    print(type(segments[0]))\n",
        "\n",
        "    # Filtra i segmenti per ottenere solo quelli esattamente di 200 righe\n",
        "    valid_segments = []\n",
        "    for segment in segments:\n",
        "        if len(segment) > 200:\n",
        "            # Mantieni solo le prime 200 righe\n",
        "            valid_segments.append(segment[:200])\n",
        "        elif len(segment) == 200:\n",
        "            # Segmento gi√† valido\n",
        "            valid_segments.append(segment)\n",
        "\n",
        "    # print(f\"Number of valid segments: {len(valid_segments)}\")\n",
        "\n",
        "    return valid_segments"
      ],
      "metadata": {
        "id": "gCfT24S7jjFB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Dataset"
      ],
      "metadata": {
        "id": "JgSM22hHk2ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "def preprocessing(segments):\n",
        "    \"\"\"\n",
        "    1) Accorpo tutti i segmenti in un unico numpy array per poter applicare la normalizzazione.\n",
        "    2) Normalizzo.\n",
        "    3) Divido di nuovo i segmenti normalizzati nel numero originario dei segmenti in input.\n",
        "    4) Ritorno i segmenti normalizzati e lo scaler utilizzato.\n",
        "    :param segments:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(f\"Tipologia: {type(segments[0])}\")\n",
        "    # Preprocessing delle feature\n",
        "    # Separiamo colonne categoriali e numeriche\n",
        "\n",
        "    categorical_columns = [0, 1, 2, 3, 6, 7, 9]\n",
        "    numerical_columns = [4, 5, 8, 10, 11]\n",
        "\n",
        "    # 1)\n",
        "    segments_array = np.vstack(segments)\n",
        "\n",
        "    print(f\"Array Unico: {segments_array}\")\n",
        "    print(f\"Tipologia: {type(segments_array)}\")\n",
        "    print(f\"Lunghezza: {len(segments_array)}\")\n",
        "    print(\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "    # 2)\n",
        "    # Configura il preprocessore con categorie globali e scaling numerico\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(categories='auto', handle_unknown='ignore'), categorical_columns),\n",
        "            ('num', MinMaxScaler(), numerical_columns)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    segments_scaled = preprocessor.fit_transform(segments_array)\n",
        "\n",
        "    print(f\"Segmenti Normalizzati: {segments_scaled[0:1]}\")\n",
        "    print(f\"Tipologia: {type(segments_scaled)}\")\n",
        "    print(f\"Lunghezza: {segments_scaled.shape}\")\n",
        "    print(\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "    # 3)\n",
        "    segments_scaled_split = np.array_split(segments_scaled.toarray(), len(segments))\n",
        "\n",
        "    print(f\"Segmenti Normalizzati e Ricostruiti: {segments_scaled_split}\")\n",
        "    print(f'Tipologia: {type(segments_scaled_split)}')\n",
        "    print(f\"Lunghezza: {len(segments_scaled_split)}\")\n",
        "    print(\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "    # 4)\n",
        "    return segments_scaled_split, preprocessor"
      ],
      "metadata": {
        "id": "w3RrffSwk-Ut"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione per il Training"
      ],
      "metadata": {
        "id": "aVvBM66XOWUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7Td_xbCkdGzf"
      },
      "outputs": [],
      "source": [
        "# 3.\n",
        "def building_and_training(segments_scaled_split):\n",
        "    # Callback per visualizzare statistiche al termine dell'addestramento\n",
        "    class TrainingSummary(Callback):\n",
        "        def on_train_end(self, logs=None):\n",
        "            print(\"\\n--- Statistiche Finali ---\")\n",
        "            print(f\"Loss finale su training set: {logs['loss']:.4f}\")\n",
        "            if 'val_loss' in logs:\n",
        "                print(f\"Loss finale su validation set: {logs['val_loss']:.4f}\")\n",
        "\n",
        "    # Definisco l'input shape\n",
        "    input_shape = segments_scaled_split[0].shape  # Forma di un segmento\n",
        "\n",
        "    # Definisco la struttura dell'autoencoder\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)  # Dropout 10%\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)  # Dropout 10%\n",
        "    encoded = Dense(64)(x)\n",
        "    encoded = LeakyReLU(alpha=0.1)(encoded)\n",
        "    x = Dense(128)(encoded)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)  # Dropout 10%\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)  # Dropout 10%\n",
        "    x = Dense(np.prod(input_shape), activation=\"sigmoid\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    decoded = Reshape(input_shape)(x)\n",
        "\n",
        "    # Creo il Modello\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Addestramento dell'autoencoder sui dati \"normali\", senza anomalie\n",
        "    history = autoencoder.fit(\n",
        "        np.array(segments_scaled_split), np.array(segments_scaled_split),\n",
        "        epochs=100, batch_size=16, shuffle=True, validation_split=0.1,\n",
        "        callbacks=[TrainingSummary(), early_stopping]\n",
        "    )\n",
        "\n",
        "    # Stampo statistiche finali direttamente dal dizionario `history.history`\n",
        "\n",
        "    print(\"\\n--- Risultati Finali ---\")\n",
        "    print(f\"Training Loss: {history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    return autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcolo dell'Errore di Ricostruzione"
      ],
      "metadata": {
        "id": "ktx1OP-vOZLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test del modello"
      ],
      "metadata": {
        "id": "QD4wdBMbOkqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esecuzione del processo"
      ],
      "metadata": {
        "id": "lUWWcpS9Opf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segments = load_and_split_data(\"normal_reduced_0005.csv\")\n",
        "print(\"Numero di segmenti:\", len(segments))\n",
        "print(segments[0].shape)"
      ],
      "metadata": {
        "id": "u9co1CiPkAkd",
        "outputId": "efb69361-c35a-4868-990d-68ed46da118e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Time:  2021-04-09 11:30:52.716203\n",
            "Finish Time:  2021-04-09 11:30:54.462284\n",
            "<class 'numpy.ndarray'>\n",
            "Numero di segmenti: 17\n",
            "(200, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_segments, preprocessor = preprocessing(segments)"
      ],
      "metadata": {
        "id": "QsahtzO7mjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = building_and_training(processed_segments)"
      ],
      "metadata": {
        "id": "_KeeyAizdmU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshols = get_rebuilding_error(autoencoder, segments)"
      ],
      "metadata": {
        "id": "dDSj0Y0S-9Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing(\"attack_1.csv\", scaler, autoencoder, threshols)"
      ],
      "metadata": {
        "id": "ZFa0kfI8_Ubp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}