{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtOhGGno1R1Ql0EL58KqPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudiaMarano/Anomaly-Detection-and-Prediction/blob/main/PhysicalDenseNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task di anomaly prediction per la parte physical tramite modello basato su Dense NN"
      ],
      "metadata": {
        "id": "RBkFcb8RTmQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "y0ofdpChfMh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1l76eETgdg",
        "outputId": "a932980b-c74a-4ab7-c1f4-1eb4be94eed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completato con successo.\n",
            "Training set: 76 segmenti\n",
            "Validation set: 9 segmenti\n",
            "Test set: 22 segmenti\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def dataset_load_and_segmentation(path_norm, att_paths=[]):\n",
        "    \"\"\"\n",
        "    Carica il dataset normale e opzionalmente i dataset di attacco, segmentandoli in finestre temporali.\n",
        "\n",
        "    Args:\n",
        "        path_norm (str): Percorso del file CSV normale.\n",
        "        att_paths (list): Lista dei percorsi dei file CSV di attacco.\n",
        "\n",
        "    Returns:\n",
        "        list: Segmenti uniformi estratti dal dataset normale e opzionalmente dai dataset di attacco.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Carico il CSV normale con diversa codifica, gestendo file con separatori differenti\n",
        "        try:\n",
        "            df_norm = pd.read_csv(path_norm, sep=',', encoding='utf-8')\n",
        "        except Exception:\n",
        "            df_norm = pd.read_csv(path_norm, sep='\\t', engine='python', encoding='utf-16')\n",
        "\n",
        "        # Rimuovo eventuali spazi nei nomi delle colonne\n",
        "        df_norm.columns = df_norm.columns.str.strip()\n",
        "\n",
        "        # Verifico la presenza della colonna Time\n",
        "        if 'Time' not in df_norm.columns:\n",
        "            raise ValueError(\"La colonna 'Time' non è presente nel file normale.\")\n",
        "\n",
        "        # Rimuovo la colonna \"Label\"\n",
        "        if 'Label' in df_norm.columns:\n",
        "            df_norm = df_norm.drop(columns=[\"Label\"])\n",
        "\n",
        "        # Converto la colonna Time in datetime\n",
        "        df_norm['Time'] = pd.to_datetime(df_norm['Time'], dayfirst=True)\n",
        "\n",
        "        # Ordino per timestamp nel caso non siano già ordinati\n",
        "        df_norm = df_norm.sort_values(by='Time')\n",
        "\n",
        "        # Definisco la durata della finestra in un minuto\n",
        "        window_duration = pd.Timedelta(minutes=1)\n",
        "\n",
        "        # Lista per i segmenti\n",
        "        segments = []\n",
        "        start_time = df_norm['Time'].iloc[0]\n",
        "\n",
        "        while start_time < df_norm['Time'].iloc[-1]:\n",
        "            end_time = start_time + window_duration\n",
        "            segment = df_norm[(df_norm['Time'] >= start_time) & (df_norm['Time'] < end_time)]\n",
        "            if len(segment) > 0:\n",
        "                segments.append(segment.drop(columns=['Time']).values)\n",
        "            start_time = end_time\n",
        "\n",
        "        # Mantengo solo i segmenti con lunghezza pari a 60\n",
        "        uniform_segments = [segment for segment in segments if len(segment) == 60]\n",
        "\n",
        "        # Aggiungo segmenti dai file di attacco se specificati\n",
        "        if att_paths:\n",
        "            for path in att_paths:\n",
        "                segments_from_att = load_from_att_file(path)\n",
        "                uniform_segments.extend(segments_from_att)\n",
        "\n",
        "        return uniform_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il caricamento e la segmentazione del dataset: {e}\")\n",
        "        return []\n",
        "\n",
        "def load_from_att_file(path):\n",
        "    \"\"\"\n",
        "    Carica segmenti uniformi da un file di attacco.\n",
        "\n",
        "    Args:\n",
        "        path (str): Percorso del file CSV di attacco.\n",
        "\n",
        "    Returns:\n",
        "        list: Segmenti uniformi estratti dal file di attacco.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Carico il CSV di attacco con diversa codifica, gestendo file con separatori differenti\n",
        "        try:\n",
        "            df_att = pd.read_csv(path, sep=',', encoding='utf-8')\n",
        "        except Exception:\n",
        "            df_att = pd.read_csv(path, sep='\\t', engine='python', encoding='utf-16')\n",
        "\n",
        "        # Rimuovo eventuali spazi nei nomi delle colonne\n",
        "        df_att.columns = df_att.columns.str.strip()\n",
        "\n",
        "        # Verifico la presenza della colonna Time\n",
        "        if 'Time' not in df_att.columns:\n",
        "            raise ValueError(f\"La colonna 'Time' non è presente nel file {path}.\")\n",
        "\n",
        "        # Converto la colonna Time in datetime\n",
        "        df_att['Time'] = pd.to_datetime(df_att['Time'], dayfirst=True)\n",
        "\n",
        "        # Ordino per timestamp nel caso non siano già ordinati\n",
        "        df_att = df_att.sort_values(by='Time')\n",
        "\n",
        "        # Definisco la durata della finestra in un minuto\n",
        "        window_duration = pd.Timedelta(minutes=1)\n",
        "\n",
        "        # Lista per i segmenti\n",
        "        segments = []\n",
        "        start_time = df_att['Time'].iloc[0]\n",
        "\n",
        "        while start_time < df_att['Time'].iloc[-1]:\n",
        "            end_time = start_time + window_duration\n",
        "            segment = df_att[(df_att['Time'] >= start_time) & (df_att['Time'] < end_time)]\n",
        "            if len(segment) > 0:\n",
        "                segments.append(segment.drop(columns=['Time']).values)\n",
        "            start_time = end_time\n",
        "\n",
        "        # Mantengo solo i segmenti con lunghezza pari a 60 e senza anomalie\n",
        "        uniform_segments = [\n",
        "            segment for segment in segments\n",
        "            if len(segment) == 60 and all(row[-1] == 'normal' for row in segment)\n",
        "        ]\n",
        "\n",
        "        # Rimuovo l'ultima colonna da ogni segmento\n",
        "        clean_segments = [segment[:, :-1] for segment in uniform_segments]\n",
        "\n",
        "        return clean_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il caricamento del file di attacco: {e}\")\n",
        "        return []\n",
        "\n",
        "def preprocessing(segments):\n",
        "    \"\"\"\n",
        "    Normalizza i segmenti forniti e restituisce i segmenti normalizzati e lo scaler.\n",
        "\n",
        "    Args:\n",
        "        segments (list): Lista di segmenti da normalizzare.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Segmenti normalizzati e scaler utilizzato.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Unisco tutti i segmenti in un unico array per la normalizzazione\n",
        "        segments_array = np.vstack(segments)\n",
        "\n",
        "        # Normalizzo i dati\n",
        "        scaler = StandardScaler()\n",
        "        segments_scaled = scaler.fit_transform(segments_array)\n",
        "\n",
        "        # Divido di nuovo i segmenti normalizzati\n",
        "        segments_scaled_split = np.array_split(segments_scaled, len(segments))\n",
        "\n",
        "        return segments_scaled_split, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il preprocessing dei segmenti: {e}\")\n",
        "        return [], None\n",
        "\n",
        "def split_dataset(segments, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Divide i segmenti in set di training, validation e test.\n",
        "\n",
        "    Args:\n",
        "        segments (list): Lista dei segmenti da dividere.\n",
        "        test_size (float): Proporzione del dataset da assegnare al test set.\n",
        "        val_size (float): Proporzione del training set da assegnare al validation set.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Training, validation e test set.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        train_val_segments, test_segments = train_test_split(segments, test_size=test_size, random_state=42)\n",
        "        train_segments, val_segments = train_test_split(train_val_segments, test_size=val_size, random_state=42)\n",
        "\n",
        "        print(f\"Training set: {len(train_segments)} segmenti\")\n",
        "        print(f\"Validation set: {len(val_segments)} segmenti\")\n",
        "        print(f\"Test set: {len(test_segments)} segmenti\")\n",
        "\n",
        "        return train_segments, val_segments, test_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la suddivisione del dataset: {e}\")\n",
        "        return [], [], []\n",
        "\n",
        "# Percorsi dei file del dataset Physical\n",
        "path_norm = \"./physical_dataset/phy_norm.csv\"\n",
        "att_paths = [\n",
        "    \"./physical_dataset/phy_att_1.csv\",\n",
        "    \"./physical_dataset/phy_att_2.csv\",\n",
        "    \"./physical_dataset/phy_att_3.csv\",\n",
        "    \"./physical_dataset/phy_att_4.csv\"\n",
        "]\n",
        "\n",
        "# Caricamento e segmentazione del dataset\n",
        "segments = dataset_load_and_segmentation(path_norm, att_paths=att_paths)\n",
        "\n",
        "# Preprocessing dei segmenti\n",
        "segments_scaled, scaler = preprocessing(segments)\n",
        "\n",
        "if segments_scaled:\n",
        "    print(\"Preprocessing completato con successo.\")\n",
        "\n",
        "    # Suddivisione del dataset\n",
        "    train_segments, val_segments, test_segments = split_dataset(segments_scaled)\n",
        "else:\n",
        "    print(\"Errore durante il preprocessing dei segmenti.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementazine del modello"
      ],
      "metadata": {
        "id": "lbWB-NYJvXHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Funzione per preparare i dati per il modello DNN\n",
        "def prepare_data_for_model(segments):\n",
        "    try:\n",
        "        X = np.array([segment[:, :-1].flatten() for segment in segments])  # Flatten delle feature\n",
        "        y = np.array([segment[0, -1] for segment in segments])  # Target dalla prima riga di ogni segmento\n",
        "\n",
        "        # Verifica che la label contenga sia anomalie (1) che normalità (0)\n",
        "        unique_labels = np.unique(y)\n",
        "        if len(unique_labels) < 2:\n",
        "            raise ValueError(\"La colonna 'label_n' non contiene sia anomalie che normalità. Verifica i dati.\")\n",
        "\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la preparazione dei dati per il modello: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Funzione per definire e addestrare il modello DNN\n",
        "def train_dnn(X_train, y_train, X_val, y_val):\n",
        "    try:\n",
        "        input_dim = X_train.shape[1]  # Calcolo della dimensione dell'input\n",
        "\n",
        "        # Definizione del modello\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_dim=input_dim),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compilazione del modello\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # Early stopping per evitare overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # Addestramento del modello\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                            epochs=50, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "        print(\"Modello addestrato con successo.\")\n",
        "        return model, history\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'addestramento del modello: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Funzione per valutare il modello\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    try:\n",
        "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        if len(np.unique(y_test)) > 1:\n",
        "            auc = roc_auc_score(y_test, y_pred)\n",
        "            print(f\"\\nROC-AUC Score: {auc:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nROC-AUC Score non calcolabile: una sola classe presente in y_test.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la valutazione del modello: {e}\")\n",
        "\n",
        "# Suddivisione bilanciata dei dati in train, validation e test\n",
        "def split_data_balanced(X, y, test_size=0.2, val_size=0.1):\n",
        "    try:\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size / (1 - test_size), stratify=y_train_val, random_state=42)\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la suddivisione bilanciata dei dati: {e}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Verifica presenza anomalie nel dataset\n",
        "def verify_anomalies_presence(segments):\n",
        "    try:\n",
        "        labels = [segment[0, -1] for segment in segments]\n",
        "        unique_labels = np.unique(labels)\n",
        "        if len(unique_labels) < 2:\n",
        "            raise ValueError(\"Il dataset non contiene sia anomalie che normalità. Verifica il preprocessing.\")\n",
        "        print(\"Verifica completata: Il dataset contiene sia anomalie che normalità.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la verifica delle anomalie: {e}\")\n",
        "\n",
        "# Preparazione dei dati per il modello\n",
        "try:\n",
        "    verify_anomalies_presence(train_segments + val_segments + test_segments)\n",
        "\n",
        "    X, y = prepare_data_for_model(train_segments + val_segments + test_segments)\n",
        "\n",
        "    # Verifica che i dati siano stati preparati correttamente\n",
        "    if X is not None and y is not None:\n",
        "        # Suddivisione dei dati in train, validation e test set\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = split_data_balanced(X, y)\n",
        "\n",
        "        if X_train is not None and X_val is not None and X_test is not None:\n",
        "            # Addestramento del modello\n",
        "            model, history = train_dnn(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Valutazione del modello\n",
        "            if model is not None:\n",
        "                evaluate_model(model, X_test, y_test)\n",
        "        else:\n",
        "            print(\"Errore nella suddivisione dei dati in train, validation e test set.\")\n",
        "    else:\n",
        "        print(\"Errore nella preparazione dei dati per il modello.\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore generale durante la pipeline: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdb1c29vvZVU",
        "outputId": "a0dabd6c-3cd1-45df-b92c-dc3e397c8577"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errore durante la verifica delle anomalie: Il dataset non contiene sia anomalie che normalità. Verifica il preprocessing.\n",
            "Errore durante la preparazione dei dati per il modello: La colonna 'label_n' non contiene sia anomalie che normalità. Verifica i dati.\n",
            "Errore nella preparazione dei dati per il modello.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def dataset_load_and_segmentation(path_norm, att_paths=[]):\n",
        "    try:\n",
        "        # Carico il CSV normale con diversa codifica, gestendo file con separatori differenti\n",
        "        try:\n",
        "            df_norm = pd.read_csv(path_norm, sep=',', encoding='utf-8')\n",
        "        except Exception:\n",
        "            df_norm = pd.read_csv(path_norm, sep='\\t', engine='python', encoding='utf-16')\n",
        "\n",
        "        # Rimuovo eventuali spazi nei nomi delle colonne\n",
        "        df_norm.columns = df_norm.columns.str.strip()\n",
        "\n",
        "        # Verifico la presenza della colonna Time\n",
        "        if 'Time' not in df_norm.columns:\n",
        "            raise ValueError(\"La colonna 'Time' non è presente nel file normale.\")\n",
        "\n",
        "        # Rimuovo la colonna \"Label\"\n",
        "        if 'Label' in df_norm.columns:\n",
        "            df_norm = df_norm.drop(columns=[\"Label\"])\n",
        "\n",
        "        # Converto la colonna Time in datetime\n",
        "        df_norm['Time'] = pd.to_datetime(df_norm['Time'], dayfirst=True)\n",
        "\n",
        "        # Ordino per timestamp nel caso non siano già ordinati\n",
        "        df_norm = df_norm.sort_values(by='Time')\n",
        "\n",
        "        # Filtro solo valori validi (0 e 1) in label_n\n",
        "        if 'label_n' in df_norm.columns:\n",
        "            unique_labels = df_norm['label_n'].unique()\n",
        "            print(f\"Valori unici trovati in label_n (normale): {unique_labels}\")\n",
        "            df_norm = df_norm[df_norm['label_n'].isin([0, 1])]\n",
        "\n",
        "        # Definisco la durata della finestra in un minuto\n",
        "        window_duration = pd.Timedelta(minutes=1)\n",
        "\n",
        "        # Lista per i segmenti\n",
        "        segments = []\n",
        "        start_time = df_norm['Time'].iloc[0]\n",
        "\n",
        "        while start_time < df_norm['Time'].iloc[-1]:\n",
        "            end_time = start_time + window_duration\n",
        "            segment = df_norm[(df_norm['Time'] >= start_time) & (df_norm['Time'] < end_time)]\n",
        "            if len(segment) > 0:\n",
        "                segments.append(segment.drop(columns=['Time']).values)\n",
        "            start_time = end_time\n",
        "\n",
        "        # Mantengo solo i segmenti con lunghezza pari a 60\n",
        "        uniform_segments = [segment for segment in segments if len(segment) == 60]\n",
        "\n",
        "        # Aggiungo segmenti dai file di attacco se specificati\n",
        "        if att_paths:\n",
        "            for path in att_paths:\n",
        "                segments_from_att = load_from_att_file(path, df_norm.columns)\n",
        "                uniform_segments.extend(segments_from_att)\n",
        "\n",
        "        return uniform_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il caricamento e la segmentazione del dataset: {e}\")\n",
        "        return []\n",
        "\n",
        "def load_from_att_file(path, reference_columns):\n",
        "    try:\n",
        "        # Carico il CSV di attacco con diversa codifica, gestendo file con separatori differenti\n",
        "        try:\n",
        "            df_att = pd.read_csv(path, sep=',', encoding='utf-8')\n",
        "        except Exception:\n",
        "            df_att = pd.read_csv(path, sep='\\t', engine='python', encoding='utf-16')\n",
        "\n",
        "        # Rimuovo eventuali spazi nei nomi delle colonne\n",
        "        df_att.columns = df_att.columns.str.strip()\n",
        "\n",
        "        # Verifico la presenza della colonna Time\n",
        "        if 'Time' not in df_att.columns:\n",
        "            raise ValueError(f\"La colonna 'Time' non è presente nel file {path}.\")\n",
        "\n",
        "        # Converto la colonna Time in datetime\n",
        "        df_att['Time'] = pd.to_datetime(df_att['Time'], dayfirst=True)\n",
        "\n",
        "        # Ordino per timestamp nel caso non siano già ordinati\n",
        "        df_att = df_att.sort_values(by='Time')\n",
        "\n",
        "        # Filtro solo valori validi (0 e 1) in label_n\n",
        "        if 'label_n' in df_att.columns:\n",
        "            unique_labels = df_att['label_n'].unique()\n",
        "            print(f\"Valori unici trovati in label_n (attacco): {unique_labels}\")\n",
        "            df_att = df_att[df_att['label_n'].isin([0, 1])]\n",
        "\n",
        "        # Aggiusto le colonne mancanti rispetto al dataset di riferimento\n",
        "        for col in reference_columns:\n",
        "            if col not in df_att.columns:\n",
        "                df_att[col] = 0\n",
        "        df_att = df_att[reference_columns]\n",
        "\n",
        "        # Definisco la durata della finestra in un minuto\n",
        "        window_duration = pd.Timedelta(minutes=1)\n",
        "\n",
        "        # Lista per i segmenti\n",
        "        segments = []\n",
        "        start_time = df_att['Time'].iloc[0]\n",
        "\n",
        "        while start_time < df_att['Time'].iloc[-1]:\n",
        "            end_time = start_time + window_duration\n",
        "            segment = df_att[(df_att['Time'] >= start_time) & (df_att['Time'] < end_time)]\n",
        "            if len(segment) > 0:\n",
        "                segments.append(segment.drop(columns=['Time']).values)\n",
        "            start_time = end_time\n",
        "\n",
        "        # Mantengo solo i segmenti con lunghezza pari a 60\n",
        "        uniform_segments = [segment for segment in segments if len(segment) == 60]\n",
        "\n",
        "        return uniform_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il caricamento del file di attacco: {e}\")\n",
        "        return []\n",
        "\n",
        "def preprocessing(segments):\n",
        "    try:\n",
        "        # Separare label_n dalle feature per evitare di normalizzarla\n",
        "        features_segments = [segment[:, :-1] for segment in segments]\n",
        "        labels = [segment[0, -1] for segment in segments]\n",
        "\n",
        "        # Unisco tutti i segmenti in un unico array per la normalizzazione\n",
        "        segments_array = np.vstack(features_segments)\n",
        "\n",
        "        # Normalizzo i dati\n",
        "        scaler = StandardScaler()\n",
        "        segments_scaled = scaler.fit_transform(segments_array)\n",
        "\n",
        "        # Divido di nuovo i segmenti normalizzati\n",
        "        features_scaled_split = np.array_split(segments_scaled, len(segments))\n",
        "\n",
        "        # Ricostruisco i segmenti includendo le label originali\n",
        "        segments_scaled_split = [\n",
        "            np.hstack((features_scaled, np.full((features_scaled.shape[0], 1), label)))\n",
        "            for features_scaled, label in zip(features_scaled_split, labels)\n",
        "        ]\n",
        "\n",
        "        return segments_scaled_split, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il preprocessing dei segmenti: {e}\")\n",
        "        return [], None\n",
        "\n",
        "def verify_anomalies_presence(segments):\n",
        "    try:\n",
        "        labels = [segment[0, -1] for segment in segments]\n",
        "        unique_labels = np.unique(labels)\n",
        "        print(f\"Valori unici nella colonna 'label_n' dopo preprocessing: {unique_labels}\")\n",
        "        if len(unique_labels) < 2:\n",
        "            raise ValueError(\"Il dataset non contiene sia anomalie che normalità. Verifica il preprocessing.\")\n",
        "        print(\"Verifica completata: Il dataset contiene sia anomalie che normalità.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la verifica delle anomalie: {e}\")\n",
        "\n",
        "def prepare_data_for_model(segments):\n",
        "    try:\n",
        "        X = np.array([segment[:, :-1].flatten() for segment in segments])  # Flatten delle feature\n",
        "        y = np.array([segment[0, -1] for segment in segments], dtype=int)  # Target dalla prima riga di ogni segmento\n",
        "\n",
        "        # Verifica che la label contenga sia anomalie (1) che normalità (0)\n",
        "        unique_labels = np.unique(y)\n",
        "        print(f\"Valori unici in 'y' prima del modello: {unique_labels}\")\n",
        "        if len(unique_labels) < 2:\n",
        "            raise ValueError(\"La colonna 'label_n' non contiene sia anomalie che normalità. Verifica i dati.\")\n",
        "\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la preparazione dei dati per il modello: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def split_data_balanced(X, y, test_size=0.2, val_size=0.1):\n",
        "    try:\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size / (1 - test_size), stratify=y_train_val, random_state=42)\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la suddivisione bilanciata dei dati: {e}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "def train_dnn(X_train, y_train, X_val, y_val):\n",
        "    try:\n",
        "        input_dim = X_train.shape[1]  # Calcolo della dimensione dell'input\n",
        "\n",
        "        # Definizione del modello\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_dim=input_dim),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compilazione del modello\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # Early stopping per evitare overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # Addestramento del modello\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                            epochs=50, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "        print(\"Modello addestrato con successo.\")\n",
        "        return model, history\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'addestramento del modello: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    try:\n",
        "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        if len(np.unique(y_test)) > 1:\n",
        "            auc = roc_auc_score(y_test, y_pred)\n",
        "            print(f\"\\nROC-AUC Score: {auc:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nROC-AUC Score non calcolabile: una sola classe presente in y_test.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la valutazione del modello: {e}\")\n",
        "\n",
        "# Pipeline\n",
        "path_norm = \"./physical_dataset/phy_norm.csv\"\n",
        "att_paths = [\n",
        "    \"./physical_dataset/phy_att_1.csv\",\n",
        "    \"./physical_dataset/phy_att_2.csv\",\n",
        "    \"./physical_dataset/phy_att_3.csv\",\n",
        "    \"./physical_dataset/phy_att_4.csv\"\n",
        "]\n",
        "\n",
        "# Caricamento e segmentazione del dataset\n",
        "segments = dataset_load_and_segmentation(path_norm, att_paths=att_paths)\n",
        "\n",
        "# Preprocessing dei segmenti\n",
        "segments_scaled, scaler = preprocessing(segments)\n",
        "\n",
        "if segments_scaled:\n",
        "    print(\"Preprocessing completato con successo.\")\n",
        "\n",
        "    # Verifica anomalie\n",
        "    verify_anomalies_presence(segments_scaled)\n",
        "\n",
        "    # Preparazione dei dati\n",
        "    X, y = prepare_data_for_model(segments_scaled)\n",
        "\n",
        "    if X is not None and y is not None:\n",
        "        # Suddivisione dei dati\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = split_data_balanced(X, y)\n",
        "\n",
        "        if X_train is not None and X_val is not None and X_test is not None:\n",
        "            # Addestramento del modello\n",
        "            model, history = train_dnn(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Valutazione del modello\n",
        "            if model is not None:\n",
        "                evaluate_model(model, X_test, y_test)\n",
        "        else:\n",
        "            print(\"Errore nella suddivisione dei dati in train, validation e test set.\")\n",
        "    else:\n",
        "        print(\"Errore nella preparazione dei dati per il modello.\")\n",
        "else:\n",
        "    print(\"Errore durante il preprocessing dei segmenti.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kh1EamnzY6b",
        "outputId": "7de271db-47b0-4861-bef0-1ed36d446b8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completato con successo.\n",
            "Valori unici nella colonna 'label_n' dopo preprocessing: [0. 1.]\n",
            "Verifica completata: Il dataset contiene sia anomalie che normalità.\n",
            "Valori unici in 'y' prima del modello: [0 1]\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.5581 - loss: 0.8038 - val_accuracy: 0.7778 - val_loss: 0.5580\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8590 - loss: 0.4375 - val_accuracy: 0.7778 - val_loss: 0.7725\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8338 - loss: 0.4116 - val_accuracy: 0.7778 - val_loss: 0.9017\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8445 - loss: 0.5007 - val_accuracy: 0.7778 - val_loss: 0.9274\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8421 - loss: 0.4135 - val_accuracy: 0.7222 - val_loss: 1.0493\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8497 - loss: 0.3951 - val_accuracy: 0.7778 - val_loss: 1.0805\n",
            "Modello addestrato con successo.\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31  0]\n",
            " [ 5  0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        31\n",
            "           1       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.86        36\n",
            "   macro avg       0.43      0.50      0.46        36\n",
            "weighted avg       0.74      0.86      0.80        36\n",
            "\n",
            "\n",
            "ROC-AUC Score: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}